/* -----------------------------------------------------------------------------
Copyright 2023 Kevin P. Barry

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
----------------------------------------------------------------------------- */

// Author: Kevin P. Barry [ta0kira@gmail.com]

define ZeoliteWhitespace {
  refines Tokenizer<ZeoliteParseContext, ZeoliteParsed>

  tokenizer () {
    return delegate -> #self
  }

  tokenizerName () {
    return typename<#self>().formatted()
  }

  tokenize (input, context) (token) {
    \ input.reset()
    token <- empty
    optional Char current <- input.current()
    while (`present` current && CharType.whitespace(`require` current)) {
      current <- input.forward().current()
    }
    scoped {
      String content <- input.take()
    } in if (content.size() > 0) {
      token <- ZeoliteParsed.leaf(label: tokenizerName(), content: content)
    }
  }
}

define ZeoliteLineComment {
  refines Tokenizer<ZeoliteParseContext, ZeoliteParsed>

  tokenizer () {
    return delegate -> #self
  }

  tokenizerName () {
    return typename<#self>().formatted()
  }

  tokenize (input, context) (token) {
    \ input.reset()
    token <- empty
    \ input.forward().forward()
    if (input.preview() != "//") {
      return _
    }
    optional Char current <- input.current()
    while (`present` current && !CharType.oneOf(`require` current, "\r\n")) {
      current <- input.forward().current()
    }
    token <- ZeoliteParsed.leaf(label: tokenizerName(), content: input.take())
  }
}

define ZeoliteOptionalSeparator {
  $ReadOnlyExcept[]$

  @category Tokenizer<ZeoliteParseContext, ZeoliteParsed> tokenizer <- TokenAlternatives.new()
      .append(UseNamedTokenizer.new<ZeoliteLineComment>())
      //.append(UseNamedTokenizer<ZeoliteBlockComment>())
      .append(UseNamedTokenizer.new<ZeoliteWhitespace>())

  default () {
    return delegate -> #self
  }

  parse (context, input, output) {
    \ StreamTokenizer:new(context: context, tokenizer: default()).tokenizeAll(input, output)
  }

  tokenize (input, output) {
    return delegate -> `tokenizer.tokenize`
  }
}

define ZeoliteUpperSymbol {
  refines Tokenizer<ZeoliteParseContext, ZeoliteParsed>

  tokenizer () {
    return delegate -> #self
  }

  tokenizerName () {
    return typename<#self>().formatted()
  }

  tokenize (input, _) {
    optional String content <- ZeoliteSymbol.parseUpperSymbol(input)&.take()
    if (! `present` content) {
      return empty
    } else {
      String type <-  ZeoliteSymbol.tryBuiltinCategory(`require` content)
                  <|| "ZeoliteCategoryName"
      return ZeoliteParsed.leaf(label: type, content: `require` content)
    }
  }
}

define ZeoliteLowerSymbol {
  refines Tokenizer<ZeoliteParseContext, ZeoliteParsed>

  tokenizer () {
    return delegate -> #self
  }

  tokenizerName () {
    return typename<#self>().formatted()
  }

  tokenize (input, _) {
    optional String content <- ZeoliteSymbol.parseLowerSymbol(input)&.take()
    if (! `present` content) {
      return empty
    } else {
      String type <-  ZeoliteSymbol.tryControlKeyword(`require` content)
                  <|| ZeoliteSymbol.tryTypeKeyword(`require` content)
                  <|| ZeoliteSymbol.tryContainKeyword(`require` content)
                  <|| ZeoliteSymbol.tryBuiltinCategory(`require` content)
                  <|| ZeoliteSymbol.tryBuiltinFunction(`require` content)
                  <|| ZeoliteSymbol.tryBuiltinConstant(`require` content)
                  <|| "ZeoliteFunctionOrVariableName"
      return ZeoliteParsed.leaf(label: type, content: `require` content)
    }
  }
}

define ZeoliteParamName {
  refines Tokenizer<ZeoliteParseContext, ZeoliteParsed>

  tokenizer () {
    return delegate -> #self
  }

  tokenizerName () {
    return typename<#self>().formatted()
  }

  tokenize (input, context) {
    optional String content <- ZeoliteSymbol.parseParamName(input)&.take()
    if (! `present` content) {
      return empty
    } else {
      String type <-  ZeoliteSymbol.tryBuiltinParam(`require` content)
                  <|| "ZeoliteParamName"
      return ZeoliteParsed.leaf(label: type, content: `require` content)
    }
  }
}

concrete ZeoliteTestcaseKeyword {
  defines Default
  refines Tokenizer<ZeoliteParseContext, ZeoliteParsed>
}

define ZeoliteTestcaseKeyword {
  default () {
    return delegate -> #self
  }

  tokenize (input, context) (token) {
    token <- empty
    scoped {
      optional String content <- ZeoliteSymbol.parseLowerSymbol(input)&.preview()
    } in if (! `present` content) {
      return empty
    } else {
      optional String type <- ZeoliteSymbol.tryTestcaseKeyword(`require` content)
      $Hidden[content]$
      if (`present` type) {
        token <- ZeoliteParsed.leaf(label: `require` type, content: input.take())
      }
    }
  }
}

define ZeoliteTestcase {
  $ReadOnlyExcept[]$

  refines Tokenizer<ZeoliteParseContext, ZeoliteParsed>

  @category Tokenizer<ZeoliteParseContext, ZeoliteParsed> testcaseSpecs <- TokenAlternatives.new()
      .append(ZeoliteOptionalSeparator.default())
      .append(UseNamedTokenizer.new<ZeoliteStringLiteral>())
      // TODO: Include numbers, type instances
      .append(ZeoliteTestcaseKeyword.default())

  tokenizer () {
    return delegate -> #self
  }

  tokenizerName () {
    return typename<#self>().formatted()
  }

  tokenize (input, context) (token) {
    scoped {
      optional String content <- ZeoliteSymbol.parseLowerSymbol(input)&.take()
    } in if (! `present` content || require(content) != "testcase") {
      return empty
    }

    Vector<ZeoliteParsed> subsections <- Vector<ZeoliteParsed>.new()
    // Setting this early still allows building it incrementally while also
    // allowing an early return if something is missing.
    token <- ZeoliteParsed.section(label: tokenizerName(), subsections)
    \ subsections.append(ZeoliteParsed.leaf(label: "ZeoliteContainKeyword", content: input.take()))

    // Testcase description string.
    \ ZeoliteOptionalSeparator.parse(context, input, subsections)
    // TODO: Parse string literal testcase name. Return early if missing.
    \ ZeoliteOptionalSeparator.parse(context, input, subsections)

    // Testcase specs.
    \ StreamTokenizer:new(context: context, tokenizer: testcaseSpecs).tokenizeAll(input, subsections)
  }
}

define ZeoliteStringLiteral {
  $ReadOnlyExcept[]$

  refines Tokenizer<ZeoliteParseContext, ZeoliteParsed>

  tokenizer () {
    return delegate -> #self
  }

  tokenizerName () {
    return typename<#self>().formatted()
  }

  tokenize (input, context) (token) {
    \ input.reset()
    token <- empty
    if (input.atEnd() || `require` input.current() != '"') {
      return _
    }
    Vector<ZeoliteParsed> subsections <- Vector<ZeoliteParsed>.new()
    // Setting this early still allows building it incrementally while also
    // allowing an early return if something is missing.
    token <- ZeoliteParsed.section(label: tokenizerName(), subsections)
    \ subsections.append(ZeoliteParsed.leaf(label: "ZeoliteDoubleQuote", content: input.forward().take()))

    scoped {
      optional Char current <- empty
    } in while (!input.atEnd() && `present` (current <- input.current())) {
      if (`require` current == '\"') {
        \ addCurrentChars(input, subsections)
        \ subsections.append(ZeoliteParsed.leaf(label: "ZeoliteDoubleQuote", content: input.forward().take()))
        return _
      } elif (`require` current == '\\') {
        \ addCurrentChars(input, subsections)
        if (`present` ZeoliteSymbol.parseEscapedChar(input.forward())) {
          \ subsections.append(ZeoliteParsed.leaf(label: "ZeoliteEscapedChar", content: input.take()))
        } else {
          \ subsections.append(ZeoliteParsed.leaf(label: "ZeoliteError", content: input.take()))
        }
      } else {
        \ input.forward()
      }
    }
    \ addCurrentChars(input, subsections)
  }

  @type addCurrentChars (TextStream, Append<ZeoliteParsed>) -> ()
  addCurrentChars (input, output) {
    if (input.tokenSize() > 0) {
      \ output.append(ZeoliteParsed.leaf(label: "ZeoliteQuotedChar", content: input.take()))
    }
  }
}

define ZeoliteBraceSection {
  $ReadOnlyExcept[]$

  refines Tokenizer<ZeoliteParseContext, ZeoliteParsed>

  @value optional Tokenizer<ZeoliteParseContext, ZeoliteParsed> tokenizer

  tokenizer () {
    return #self{ empty }
  }

  new (tokenizer) {
    return delegate -> #self
  }

  tokenizerName () {
    return typename<#self>().formatted()
  }

  tokenize (input, context) (token) {
    \ input.reset()
    token <- empty
    if (input.atEnd() || `require` input.current() != '{') {
      return _
    }
    Vector<ZeoliteParsed> subsections <- Vector<ZeoliteParsed>.new()
    \ subsections.append(ZeoliteParsed.leaf(label: "ZeoliteOpenBrace", content: input.forward().take()))
    \ StreamTokenizer:new(context: context, tokenizer: tokenizer <|| context.defaultTokenizer()).tokenizeAll(input, subsections)
    \ input.reset()
    if (!input.atEnd() && `require` input.current() == '}') {
      \ subsections.append(ZeoliteParsed.leaf(label: "ZeoliteCloseBrace", content: input.forward().take()))
    }
    token <- ZeoliteParsed.section(label: tokenizerName(), subsections)
  }
}
